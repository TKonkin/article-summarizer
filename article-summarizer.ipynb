{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name:  Terry Konkin  \n",
    "  \n",
    "https://github.com/TKonkin/article-summarizer\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "import requests\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------- ------------------------ 5.0/12.8 MB 26.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.0/12.8 MB 26.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 25.1 MB/s  0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# This cell is only run when the command is not successful in the terminal (occassionally).\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article HTML saved to C:\\Projects\\article-summarizer\\goldman-saks-ai-powered-new-employee.pkl\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cnbc.com/2025/07/11/goldman-sachs-autonomous-coder-pilot-marks-major-ai-milestone.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html5lib')\n",
    "\n",
    "article = soup.find('article')\n",
    "article_html = str(article)\n",
    "\n",
    "file_path = r'C:\\Projects\\article-summarizer\\goldman-saks-ai-powered-new-employee.pkl'\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(article_html, f)\n",
    "\n",
    "print(f\"Article HTML saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'google_gemini_article.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoaded article text from pickle.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTML_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     30\u001b[39m         html = f.read()\n\u001b[32m     31\u001b[39m     soup = BeautifulSoup(html, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\article-summarizer\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'google_gemini_article.html'"
     ]
    }
   ],
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "# Final Project: Article Summarizer (with Pickle Optimization)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# File paths\n",
    "HTML_FILE = \"google_gemini_article.html\"\n",
    "TEXT_PICKLE = \"article_text.pkl\"\n",
    "DOC_PICKLE = \"parsed_doc.pkl\"\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Load or create the article text\n",
    "# ---------------------------------------\n",
    "\n",
    "if os.path.exists(TEXT_PICKLE):\n",
    "    with open(TEXT_PICKLE, \"rb\") as f:\n",
    "        article_text = pickle.load(f)\n",
    "    print(\"Loaded article text from pickle.\")\n",
    "else:\n",
    "    with open(HTML_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    article_text = soup.get_text()\n",
    "    with open(TEXT_PICKLE, \"wb\") as f:\n",
    "        pickle.dump(article_text, f)\n",
    "    print(\"Parsed HTML and saved article text.\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Sentiment analysis\n",
    "# ---------------------------------------\n",
    "\n",
    "blob = TextBlob(article_text)\n",
    "print(f\"\\nPolarity Score: {blob.sentiment.polarity}\")\n",
    "print(f\"Number of Sentences: {len(blob.sentences)}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Load or create the spaCy Doc\n",
    "# ---------------------------------------\n",
    "\n",
    "if os.path.exists(DOC_PICKLE):\n",
    "    with open(DOC_PICKLE, \"rb\") as f:\n",
    "        doc = pickle.load(f)\n",
    "    print(\"Loaded spaCy doc from pickle.\")\n",
    "else:\n",
    "    doc = nlp(article_text)\n",
    "    with open(DOC_PICKLE, \"wb\") as f:\n",
    "        pickle.dump(doc, f)\n",
    "    print(\"Processed text and saved spaCy doc.\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Most frequent tokens\n",
    "# ---------------------------------------\n",
    "\n",
    "tokens = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "token_freq = Counter(tokens)\n",
    "top_tokens = token_freq.most_common(5)\n",
    "\n",
    "print(\"\\nTop 5 Tokens:\")\n",
    "for word, freq in top_tokens:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4. Most frequent lemmas\n",
    "# ---------------------------------------\n",
    "\n",
    "lemmas = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "lemma_freq = Counter(lemmas)\n",
    "top_lemmas = lemma_freq.most_common(5)\n",
    "\n",
    "print(\"\\nTop 5 Lemmas:\")\n",
    "for lemma, freq in top_lemmas:\n",
    "    print(f\"{lemma}: {freq}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5. Token score histogram\n",
    "# ---------------------------------------\n",
    "\n",
    "token_scores = []\n",
    "for sent in doc.sents:\n",
    "    score = sum(token_freq.get(token.text.lower(), 0) for token in sent if token.is_alpha and not token.is_stop)\n",
    "    token_scores.append(score)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(token_scores, bins=15)\n",
    "plt.title(\"Sentence Scores Based on Tokens\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Number of Sentences\")\n",
    "plt.savefig(\"histogram_tokens.png\")\n",
    "plt.close()\n",
    "\n",
    "# Most common token score range is 0-40\n",
    "\n",
    "# ---------------------------------------\n",
    "# 6. Lemma score histogram\n",
    "# ---------------------------------------\n",
    "\n",
    "lemma_scores = []\n",
    "for sent in doc.sents:\n",
    "    score = sum(lemma_freq.get(token.lemma_.lower(), 0) for token in sent if token.is_alpha and not token.is_stop)\n",
    "    lemma_scores.append(score)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(lemma_scores, bins=15)\n",
    "plt.title(\"Sentence Scores Based on Lemmas\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Number of Sentences\")\n",
    "plt.savefig(\"histogram_lemmas.png\")\n",
    "plt.close()\n",
    "\n",
    "# Most common lemma score range is 0-40\n",
    "\n",
    "# ---------------------------------------\n",
    "# 7. Cutoff Scores\n",
    "# ---------------------------------------\n",
    "# Cutoff Score (tokens): 40\n",
    "# Cutoff Score (lemmas): 40\n",
    "\n",
    "# ---------------------------------------\n",
    "# 8. Token-based summary\n",
    "# ---------------------------------------\n",
    "\n",
    "summary_token = [\n",
    "    sent.text.strip()\n",
    "    for sent, score in zip(doc.sents, token_scores)\n",
    "    if score > 40\n",
    "]\n",
    "token_summary_text = \" \".join(summary_token)\n",
    "\n",
    "print(\"\\nToken-Based Summary:\")\n",
    "print(token_summary_text)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 9. Token summary sentiment\n",
    "# ---------------------------------------\n",
    "\n",
    "summary_blob = TextBlob(token_summary_text)\n",
    "print(f\"\\nToken-Based Summary Polarity: {summary_blob.sentiment.polarity}\")\n",
    "print(f\"Number of Sentences in Token-Based Summary: {len(summary_token)}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 10. Lemma-based summary\n",
    "# ---------------------------------------\n",
    "\n",
    "summary_lemma = [\n",
    "    sent.text.strip()\n",
    "    for sent, score in zip(doc.sents, lemma_scores)\n",
    "    if score > 40\n",
    "]\n",
    "lemma_summary_text = \" \".join(summary_lemma)\n",
    "\n",
    "print(\"\\nLemma-Based Summary:\")\n",
    "print(lemma_summary_text)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 11. Lemma summary sentiment\n",
    "# ---------------------------------------\n",
    "\n",
    "lemma_blob = TextBlob(lemma_summary_text)\n",
    "print(f\"\\nLemma-Based Summary Polarity: {lemma_blob.sentiment.polarity}\")\n",
    "print(f\"Number of Sentences in Lemma-Based Summary: {len(summary_lemma)}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 12. Comparison of polarity scores\n",
    "# ---------------------------------------\n",
    "# Full Article Polarity: ~blob.sentiment.polarity~\n",
    "# Token Summary Polarity: ~summary_blob.sentiment.polarity~\n",
    "# Lemma Summary Polarity: ~lemma_blob.sentiment.polarity~\n",
    "# Summary versions tend to highlight key ideas and emotional content, raising polarity scores.\n",
    "\n",
    "# ---------------------------------------\n",
    "# 13. Best summary and why\n",
    "# ---------------------------------------\n",
    "# The token-based summary feels more coherent and information-dense.\n",
    "# It better captures the key topics discussed in the article.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Polarity: 0.000\n"
     ]
    }
   ],
   "source": [
    "# polarity score only\n",
    "\n",
    "file_path = r\"C:\\Projects\\article-summarizer\\goldman-saks-ai-powered-new-employee.pkl\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"spacytextblob\")\n",
    "\n",
    "html_content = data[0]  \n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "for tag in soup(['script', 'style']):\n",
    "    tag.decompose()\n",
    "\n",
    "article_text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "doc = nlp(article_text)\n",
    "\n",
    "print(f\"Article Polarity: {doc._.blob.polarity:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences: 1\n"
     ]
    }
   ],
   "source": [
    "# number of sentences\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"spacytextblob\")\n",
    "\n",
    "file_path = r\"C:\\Projects\\article-summarizer\\goldman-saks-ai-powered-new-employee.pkl\"\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "html_content = data[0]\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "for tag in soup(['script', 'style']):\n",
    "    tag.decompose()\n",
    "article_text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "doc = nlp(article_text)\n",
    "\n",
    "print(f\"Number of Sentences: {len(list(doc.sents))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
